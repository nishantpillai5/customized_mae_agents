{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80c149f4-d3da-4c60-a960-a269c14def3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Convolution2D, Permute, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d658a9e-1ccc-4e56-9846-5dd08309b57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_2 (Flatten)         (None, 40)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                1312      \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 32)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 5)                 165       \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 5)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,477\n",
      "Trainable params: 1,477\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "INPUT_SHAPE = (10,)\n",
    "WINDOW_LENGTH = 4\n",
    "nb_actions = 5\n",
    "\n",
    "input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
    "model = Sequential()\n",
    "\n",
    "# (width, height, channels)\n",
    "#model.add(Permute((2, 1), input_shape=input_shape))\n",
    "model.add(Input(shape=(4, 10)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63feb355-d3e2-419c-991e-85d91eb8ac9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        print(\"obs\", observation)\n",
    "        return np.array(observation)\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        # We could perform this processing step in `process_observation`. In this case, however,\n",
    "        # we would need to store a `float32` array instead, which is 4x more memory intensive than\n",
    "        # an `uint8` array. This matters if we store 1M observations.\n",
    "        # processed_batch = batch.astype('float32') / 255.\n",
    "        return batch\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        return np.clip(reward, -1., 1.)\n",
    "\n",
    "processor = myProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e251a625-aacc-4816-af24-83a918c054d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05,\n",
    "                              nb_steps=1000000)\n",
    "memory = SequentialMemory(limit=1000000, window_length=WINDOW_LENGTH)\n",
    "\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, policy=policy, memory=memory,\n",
    "               processor=processor, nb_steps_warmup=50000, gamma=.99, target_model_update=10000,\n",
    "               train_interval=4, delta_clip=1.)\n",
    "dqn.compile(Adam(learning_rate=.00025), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07b3db0f-0f19-47e5-b437-f37e30484013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.forward(np.random.normal(size=(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ef8897a-e0c6-4041-a3dc-6d2847e7b59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cycles = 20\n",
    "\n",
    "def env_creator(render_mode=\"rgb_array\", cycles=200):\n",
    "    from src.world import world_utils\n",
    "    env = world_utils.env(render_mode=render_mode, max_cycles=cycles)\n",
    "    return env\n",
    "\n",
    "env = env_creator(render_mode=\"rgb_array\", cycles=cycles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cbe20ec-a3c2-4b7b-9d66-95fee6151f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Okay, now it's time to learn something! We capture the interrupt exception so that training\n",
    "    # can be prematurely aborted. Notice that now you can use the built-in tensorflow.keras callbacks!\n",
    "    weights_filename = f'dqn_noenv_weights.h5f'\n",
    "    checkpoint_weights_filename = 'dqn_noenv_weights_{step}.h5f'\n",
    "    log_filename = f'dqn_noenv_log.json'\n",
    "    callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=250000)]\n",
    "    callbacks += [FileLogger(log_filename, interval=100)]\n",
    "    dqn.fit(env, callbacks=callbacks, nb_steps=1750000, log_interval=10000)\n",
    "\n",
    "    # After training is done, we save the final weights one more time.\n",
    "    dqn.save_weights(weights_filename, overwrite=True)\n",
    "\n",
    "    # Finally, evaluate our algorithm for 10 episodes.\n",
    "    dqn.test(env, nb_episodes=10, visualize=False)\n",
    "\n",
    "def test():\n",
    "    weights_filename = f'dqn_{args.env_name}_weights.h5f'\n",
    "    if args.weights:\n",
    "        weights_filename = args.weights\n",
    "    dqn.load_weights(weights_filename)\n",
    "    dqn.test(env, nb_episodes=10, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df19c483-194b-4131-b337-09a8109e107b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1750000 steps ...\n",
      "obs None\n",
      "Interval 1 (0 steps performed)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_1 to have 3 dimensions, but got array with shape (1, 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 9\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [ModelIntervalCheckpoint(checkpoint_weights_filename, interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m250000\u001b[39m)]\n\u001b[0;32m      8\u001b[0m callbacks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [FileLogger(log_filename, interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)]\n\u001b[1;32m----> 9\u001b[0m \u001b[43mdqn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1750000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# After training is done, we save the final weights one more time.\u001b[39;00m\n\u001b[0;32m     12\u001b[0m dqn\u001b[38;5;241m.\u001b[39msave_weights(weights_filename, overwrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mD:\\Study_Documents\\thesis\\env\\lib\\site-packages\\rl\\core.py:168\u001b[0m, in \u001b[0;36mAgent.fit\u001b[1;34m(self, env, nb_steps, action_repetition, callbacks, verbose, visualize, nb_max_start_steps, start_step_policy, log_interval, nb_max_episode_steps)\u001b[0m\n\u001b[0;32m    165\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_step_begin(episode_step)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;66;03m# This is were all of the work happens. We first perceive and compute the action\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;66;03m# (forward step) and then use the reward to improve (backward step).\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor\u001b[38;5;241m.\u001b[39mprocess_action(action)\n",
      "File \u001b[1;32mD:\\Study_Documents\\thesis\\env\\lib\\site-packages\\rl\\agents\\dqn.py:224\u001b[0m, in \u001b[0;36mDQNAgent.forward\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, observation):\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# Select an action.\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mget_recent_state(observation)\n\u001b[1;32m--> 224\u001b[0m     q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_q_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m    226\u001b[0m         action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mselect_action(q_values\u001b[38;5;241m=\u001b[39mq_values)\n",
      "File \u001b[1;32mD:\\Study_Documents\\thesis\\env\\lib\\site-packages\\rl\\agents\\dqn.py:68\u001b[0m, in \u001b[0;36mAbstractDQNAgent.compute_q_values\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_q_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[1;32m---> 68\u001b[0m     q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_batch_q_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m q_values\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb_actions,)\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m q_values\n",
      "File \u001b[1;32mD:\\Study_Documents\\thesis\\env\\lib\\site-packages\\rl\\agents\\dqn.py:63\u001b[0m, in \u001b[0;36mAbstractDQNAgent.compute_batch_q_values\u001b[1;34m(self, state_batch)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_batch_q_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, state_batch):\n\u001b[0;32m     62\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_state_batch(state_batch)\n\u001b[1;32m---> 63\u001b[0m     q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m q_values\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (\u001b[38;5;28mlen\u001b[39m(state_batch), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb_actions)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m q_values\n",
      "File \u001b[1;32mD:\\Study_Documents\\thesis\\env\\lib\\site-packages\\keras\\engine\\training_v1.py:1303\u001b[0m, in \u001b[0;36mModel.predict_on_batch\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m   1298\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m   1299\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`predict_on_batch` is not supported for models distributed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1300\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith tf.distribute.Strategy.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1301\u001b[0m     )\n\u001b[0;32m   1302\u001b[0m \u001b[38;5;66;03m# Validate and standardize user data.\u001b[39;00m\n\u001b[1;32m-> 1303\u001b[0m inputs, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_standardize_user_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextract_tensors_from_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m   1305\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;66;03m# If `self._distribution_strategy` is True, then we are in a replica\u001b[39;00m\n\u001b[0;32m   1307\u001b[0m \u001b[38;5;66;03m# context at this point.\u001b[39;00m\n\u001b[0;32m   1308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_eagerly \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribution_strategy:\n",
      "File \u001b[1;32mD:\\Study_Documents\\thesis\\env\\lib\\site-packages\\keras\\engine\\training_v1.py:2650\u001b[0m, in \u001b[0;36mModel._standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2641\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2642\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m run_eagerly\n\u001b[0;32m   2643\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_build_called\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2646\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(_is_symbolic_tensor(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m all_inputs)\n\u001b[0;32m   2647\u001b[0m ):\n\u001b[0;32m   2648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [], [], \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2650\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_standardize_tensors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2652\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2653\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2654\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_eagerly\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_eagerly\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdict_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdict_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2659\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Study_Documents\\thesis\\env\\lib\\site-packages\\keras\\engine\\training_v1.py:2691\u001b[0m, in \u001b[0;36mModel._standardize_tensors\u001b[1;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[0;32m   2688\u001b[0m \u001b[38;5;66;03m# Standardize the inputs.\u001b[39;00m\n\u001b[0;32m   2689\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, (tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset, tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset)):\n\u001b[0;32m   2690\u001b[0m     \u001b[38;5;66;03m# TODO(fchollet): run static checks with dataset output shape(s).\u001b[39;00m\n\u001b[1;32m-> 2691\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_utils_v1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstandardize_input_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeed_input_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeed_input_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Don't enforce the batch size.\u001b[39;49;00m\n\u001b[0;32m   2696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2697\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2699\u001b[0m \u001b[38;5;66;03m# Get typespecs for the input data and sanitize it if necessary.\u001b[39;00m\n\u001b[0;32m   2700\u001b[0m \u001b[38;5;66;03m# TODO(momernick): This should be capable of doing full input validation\u001b[39;00m\n\u001b[0;32m   2701\u001b[0m \u001b[38;5;66;03m# at all times - validate that this is so and refactor the\u001b[39;00m\n\u001b[0;32m   2702\u001b[0m \u001b[38;5;66;03m# standardization code.\u001b[39;00m\n\u001b[0;32m   2703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset):\n",
      "File \u001b[1;32mD:\\Study_Documents\\thesis\\env\\lib\\site-packages\\keras\\engine\\training_utils_v1.py:712\u001b[0m, in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    710\u001b[0m shape \u001b[38;5;241m=\u001b[39m shapes[i]\n\u001b[0;32m    711\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data_shape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(shape):\n\u001b[1;32m--> 712\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    713\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError when checking \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    714\u001b[0m         \u001b[38;5;241m+\u001b[39m exception_prefix\n\u001b[0;32m    715\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: expected \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    716\u001b[0m         \u001b[38;5;241m+\u001b[39m names[i]\n\u001b[0;32m    717\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m to have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    718\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(shape))\n\u001b[0;32m    719\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m dimensions, but got array with shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    720\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(data_shape)\n\u001b[0;32m    721\u001b[0m     )\n\u001b[0;32m    722\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_batch_axis:\n\u001b[0;32m    723\u001b[0m     data_shape \u001b[38;5;241m=\u001b[39m data_shape[\u001b[38;5;241m1\u001b[39m:]\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected input_1 to have 3 dimensions, but got array with shape (1, 4)"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46365abf-2463-424e-bc02-c12ef10c112b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
