{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "407a02fc-f610-4887-9689-b65caa01cf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Uses Ray's RLLib to train agents to play Pistonball.\n",
    "\n",
    "Author: Rohan (https://github.com/Rohan138)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "#import supersuit as ss\n",
    "from ray import tune\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.env.wrappers.pettingzoo_env import ParallelPettingZooEnv\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.tune.registry import register_env\n",
    "from torch import nn\n",
    "\n",
    "from pettingzoo.mpe import simple_tag_v2\n",
    "from pettingzoo.test import render_test\n",
    "from pettingzoo.test import performance_benchmark\n",
    "from pettingzoo.test import test_save_obs\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "#raise NotImplementedError(\n",
    "#    \"There are currently bugs in this tutorial, we will fix them soon.\"\n",
    "#)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9b8bb82-25a2-4e11-b50a-7d9bb9e1cf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNNModelV2(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, act_space, num_outputs, *args, **kwargs):\n",
    "        TorchModelV2.__init__(self, obs_space, act_space, num_outputs, *args, **kwargs)\n",
    "        nn.Module.__init__(self)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, [8, 8], stride=(4, 4)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, [4, 4], stride=(2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, [3, 3], stride=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            (nn.Linear(3136, 512)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.policy_fn = nn.Linear(512, num_outputs)\n",
    "        self.value_fn = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        model_out = self.model(input_dict[\"obs\"].permute(0, 3, 1, 2))\n",
    "        self._value_out = self.value_fn(model_out)\n",
    "        return self.policy_fn(model_out), state\n",
    "\n",
    "    def value_function(self):\n",
    "        return self._value_out.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db51c000-5834-48bc-a2a8-8cb4f3ae9bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def env_creator(render_mode=\"rgb_array\", cycles=200):\n",
    "    \n",
    "    from src.world import world_utils\n",
    "    env = world_utils.env(render_mode=render_mode, max_cycles=cycles)\n",
    "\n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bf74215-390b-4dc2-9d57-6bfbdc0bca3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cycles = 800\n",
    "env = env_creator(cycles=cycles)\n",
    "env.reset()\n",
    "current_cycle = 0\n",
    "agent_count = 4  # todo: get from env\n",
    "action_queue = []\n",
    "\n",
    "for agent in env.agent_iter():\n",
    "    if current_cycle >= cycles * agent_count:\n",
    "        break\n",
    "    if current_cycle % agent_count == 0:\n",
    "        adversary_0_action = random.choice([0, 1, 2, 3, 4])\n",
    "        adversary_1_action = random.choice([0, 1, 2, 3, 4])\n",
    "        adversary_2_action = random.choice([0, 1, 2, 3, 4])\n",
    "        good_agent_action = random.choice([0, 1, 2, 3, 4])\n",
    "\n",
    "        action_queue += [\n",
    "            adversary_0_action,\n",
    "            adversary_1_action,\n",
    "            adversary_2_action,\n",
    "            good_agent_action\n",
    "        ]\n",
    "    # print(agent)\n",
    "    env.render()\n",
    "    # obs, reward, done, info = env.last()\n",
    "    observation, cumulative_rewards, terminations, truncations, infos = env.last()\n",
    "    action = action_queue.pop(0)\n",
    "    env.step(action)\n",
    "    current_cycle += 1\n",
    "\n",
    "    # Following this but it's not working: https://github.com/openai/multiagent-particle-envs/issues/76\n",
    "    # score+=reward\n",
    "else:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25cb42d3-7f30-4c58-a1ad-d56987defd1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-03-10 17:44:27</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:15.51        </td></tr>\n",
       "<tr><td>Memory:      </td><td>10.6/15.9 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 0/12 CPUs, 0/1 GPUs, 0.0/4.49 GiB heap, 0.0/2.25 GiB objects\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pistonball_v6_c9148_00000</td><td style=\"text-align: right;\">           1</td><td>C:\\Users\\rober\\ray_results\\pistonball_v6\\PPO\\PPO_pistonball_v6_c9148_00000_0_2023-03-10_17-44-12\\error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pistonball_v6_c9148_00000</td><td>ERROR   </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m 2023-03-10 17:44:17,971\tWARNING algorithm_config.py:596 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m 2023-03-10 17:44:18,730\tINFO algorithm.py:506 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2023-03-10 17:44:27,502\tERROR trial_runner.py:1062 -- Trial PPO_pistonball_v6_c9148_00000: Error processing event.\n",
      "ray.tune.error._TuneNoNextExecutorEventError: Traceback (most recent call last):\n",
      "  File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\tune\\execution\\ray_trial_executor.py\", line 1276, in get_next_executor_event\n",
      "    future_result = ray.get(ready_future)\n",
      "  File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\_private\\worker.py\", line 2382, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::PPO.__init__()\u001b[39m (pid=25428, ip=127.0.0.1, repr=PPO)\n",
      "  File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\", line 240, in _setup\n",
      "    self.add_workers(\n",
      "  File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\", line 614, in add_workers\n",
      "    raise result.get()\n",
      "  File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\rllib\\utils\\actor_manager.py\", line 477, in __fetch_result\n",
      "    result = ray.get(r)\n",
      "  File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\_private\\worker.py\", line 2382, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=11700, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x00000147939BB5E0>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 803, in ray._raylet.execute_task.function_executor\n",
      "  File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 607, in __init__\n",
      "    self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "  File \"C:\\Users\\rober\\AppData\\Local\\Temp\\ipykernel_10168\\2798445761.py\", line 4, in <lambda>\n",
      "  File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\rllib\\env\\wrappers\\pettingzoo_env.py\", line 184, in __init__\n",
      "    assert all(\n",
      "AssertionError: Observation spaces for all agents must be identical. Perhaps SuperSuit's pad_observations wrapper can help (useage: `supersuit.aec_wrappers.pad_observations(env)`\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::PPO.__init__()\u001b[39m (pid=25428, ip=127.0.0.1, repr=PPO)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 850, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 902, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 803, in ray._raylet.execute_task.function_executor\n",
      "  File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py\", line 445, in __init__\n",
      "    super().__init__(\n",
      "  File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\tune\\trainable\\trainable.py\", line 169, in __init__\n",
      "    self.setup(copy.deepcopy(self.config))\n",
      "  File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py\", line 571, in setup\n",
      "    self.workers = WorkerSet(\n",
      "  File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\", line 192, in __init__\n",
      "    raise e.args[0].args[2]\n",
      "AssertionError: Observation spaces for all agents must be identical. Perhaps SuperSuit's pad_observations wrapper can help (useage: `supersuit.aec_wrappers.pad_observations(env)`\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>trial_id   </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pistonball_v6_c9148_00000</td><td>c9148_00000</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m 2023-03-10 17:44:27,481\tERROR actor_manager.py:496 -- Ray error, taking actor 1 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=11700, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x00000147939BB5E0>)\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 803, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 607, in __init__\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"C:\\Users\\rober\\AppData\\Local\\Temp\\ipykernel_10168\\2798445761.py\", line 4, in <lambda>\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\rllib\\env\\wrappers\\pettingzoo_env.py\", line 184, in __init__\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m     assert all(\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m AssertionError: Observation spaces for all agents must be identical. Perhaps SuperSuit's pad_observations wrapper can help (useage: `supersuit.aec_wrappers.pad_observations(env)`\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m 2023-03-10 17:44:27,482\tERROR actor_manager.py:496 -- Ray error, taking actor 2 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=10088, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000001B580F4B5E0>)\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 803, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 607, in __init__\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"C:\\Users\\rober\\AppData\\Local\\Temp\\ipykernel_10168\\2798445761.py\", line 4, in <lambda>\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\rllib\\env\\wrappers\\pettingzoo_env.py\", line 184, in __init__\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m     assert all(\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m AssertionError: Observation spaces for all agents must be identical. Perhaps SuperSuit's pad_observations wrapper can help (useage: `supersuit.aec_wrappers.pad_observations(env)`\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m 2023-03-10 17:44:27,482\tERROR actor_manager.py:496 -- Ray error, taking actor 3 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=4372, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000001DF0DB29640>)\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 803, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 607, in __init__\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"C:\\Users\\rober\\AppData\\Local\\Temp\\ipykernel_10168\\2798445761.py\", line 4, in <lambda>\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\rllib\\env\\wrappers\\pettingzoo_env.py\", line 184, in __init__\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m     assert all(\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m AssertionError: Observation spaces for all agents must be identical. Perhaps SuperSuit's pad_observations wrapper can help (useage: `supersuit.aec_wrappers.pad_observations(env)`\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m 2023-03-10 17:44:27,482\tERROR actor_manager.py:496 -- Ray error, taking actor 4 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=12752, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000001CA7F97B610>)\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 803, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 607, in __init__\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"C:\\Users\\rober\\AppData\\Local\\Temp\\ipykernel_10168\\2798445761.py\", line 4, in <lambda>\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\rllib\\env\\wrappers\\pettingzoo_env.py\", line 184, in __init__\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m     assert all(\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m AssertionError: Observation spaces for all agents must be identical. Perhaps SuperSuit's pad_observations wrapper can help (useage: `supersuit.aec_wrappers.pad_observations(env)`\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m 2023-03-10 17:44:27,487\tERROR worker.py:772 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPO.__init__()\u001b[39m (pid=25428, ip=127.0.0.1, repr=PPO)\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\", line 240, in _setup\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m     self.add_workers(\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\", line 614, in add_workers\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m     raise result.get()\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\rllib\\utils\\actor_manager.py\", line 477, in __fetch_result\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m     result = ray.get(r)\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\_private\\worker.py\", line 2382, in get\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m     raise value\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=11700, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x00000147939BB5E0>)\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 803, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 607, in __init__\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"C:\\Users\\rober\\AppData\\Local\\Temp\\ipykernel_10168\\2798445761.py\", line 4, in <lambda>\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\rllib\\env\\wrappers\\pettingzoo_env.py\", line 184, in __init__\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m     assert all(\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m AssertionError: Observation spaces for all agents must be identical. Perhaps SuperSuit's pad_observations wrapper can help (useage: `supersuit.aec_wrappers.pad_observations(env)`\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m \u001b[36mray::PPO.__init__()\u001b[39m (pid=25428, ip=127.0.0.1, repr=PPO)\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 850, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 902, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 803, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py\", line 445, in __init__\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m     super().__init__(\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\tune\\trainable\\trainable.py\", line 169, in __init__\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py\", line 571, in setup\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m   File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\", line 192, in __init__\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m     raise e.args[0].args[2]\n",
      "\u001b[2m\u001b[36m(PPO pid=25428)\u001b[0m AssertionError: Observation spaces for all agents must be identical. Perhaps SuperSuit's pad_observations wrapper can help (useage: `supersuit.aec_wrappers.pad_observations(env)`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=12752)\u001b[0m 2023-03-10 17:44:27,433\tERROR worker.py:772 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=12752, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000001CA7F97B610>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=12752)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=12752)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=12752)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 803, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=12752)\u001b[0m   File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=12752)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=12752)\u001b[0m   File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=12752)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=12752)\u001b[0m   File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 607, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=12752)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=12752)\u001b[0m   File \"C:\\Users\\rober\\AppData\\Local\\Temp\\ipykernel_10168\\2798445761.py\", line 4, in <lambda>\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=12752)\u001b[0m   File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\rllib\\env\\wrappers\\pettingzoo_env.py\", line 184, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=12752)\u001b[0m     assert all(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=12752)\u001b[0m AssertionError: Observation spaces for all agents must be identical. Perhaps SuperSuit's pad_observations wrapper can help (useage: `supersuit.aec_wrappers.pad_observations(env)`\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10088)\u001b[0m 2023-03-10 17:44:27,426\tERROR worker.py:772 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=10088, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000001B580F4B5E0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10088)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10088)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10088)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 803, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10088)\u001b[0m   File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10088)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10088)\u001b[0m   File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10088)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10088)\u001b[0m   File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 607, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10088)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10088)\u001b[0m   File \"C:\\Users\\rober\\AppData\\Local\\Temp\\ipykernel_10168\\2798445761.py\", line 4, in <lambda>\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10088)\u001b[0m   File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\rllib\\env\\wrappers\\pettingzoo_env.py\", line 184, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10088)\u001b[0m     assert all(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10088)\u001b[0m AssertionError: Observation spaces for all agents must be identical. Perhaps SuperSuit's pad_observations wrapper can help (useage: `supersuit.aec_wrappers.pad_observations(env)`\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11700)\u001b[0m 2023-03-10 17:44:27,465\tERROR worker.py:772 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=11700, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x00000147939BB5E0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11700)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11700)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11700)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 803, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11700)\u001b[0m   File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11700)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11700)\u001b[0m   File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11700)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11700)\u001b[0m   File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 607, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11700)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11700)\u001b[0m   File \"C:\\Users\\rober\\AppData\\Local\\Temp\\ipykernel_10168\\2798445761.py\", line 4, in <lambda>\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11700)\u001b[0m   File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\rllib\\env\\wrappers\\pettingzoo_env.py\", line 184, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11700)\u001b[0m     assert all(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11700)\u001b[0m AssertionError: Observation spaces for all agents must be identical. Perhaps SuperSuit's pad_observations wrapper can help (useage: `supersuit.aec_wrappers.pad_observations(env)`\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4372)\u001b[0m 2023-03-10 17:44:27,471\tERROR worker.py:772 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=4372, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000001DF0DB29640>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4372)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4372)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4372)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 803, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4372)\u001b[0m   File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4372)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4372)\u001b[0m   File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4372)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4372)\u001b[0m   File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 607, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4372)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4372)\u001b[0m   File \"C:\\Users\\rober\\AppData\\Local\\Temp\\ipykernel_10168\\2798445761.py\", line 4, in <lambda>\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4372)\u001b[0m   File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\rllib\\env\\wrappers\\pettingzoo_env.py\", line 184, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4372)\u001b[0m     assert all(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4372)\u001b[0m AssertionError: Observation spaces for all agents must be identical. Perhaps SuperSuit's pad_observations wrapper can help (useage: `supersuit.aec_wrappers.pad_observations(env)`\n",
      "2023-03-10 17:44:27,549\tERROR ray_trial_executor.py:930 -- An exception occurred when trying to stop the Ray actor:Traceback (most recent call last):\n",
      "  File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\tune\\execution\\ray_trial_executor.py\", line 921, in _resolve_stop_event\n",
      "    ray.get(future, timeout=timeout)\n",
      "  File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\_private\\worker.py\", line 2382, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::PPO.__init__()\u001b[39m (pid=25428, ip=127.0.0.1, repr=PPO)\n",
      "  File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\", line 240, in _setup\n",
      "    self.add_workers(\n",
      "  File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\", line 614, in add_workers\n",
      "    raise result.get()\n",
      "  File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\rllib\\utils\\actor_manager.py\", line 477, in __fetch_result\n",
      "    result = ray.get(r)\n",
      "  File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\_private\\worker.py\", line 2382, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=11700, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x00000147939BB5E0>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 803, in ray._raylet.execute_task.function_executor\n",
      "  File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 607, in __init__\n",
      "    self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "  File \"C:\\Users\\rober\\AppData\\Local\\Temp\\ipykernel_10168\\2798445761.py\", line 4, in <lambda>\n",
      "  File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\rllib\\env\\wrappers\\pettingzoo_env.py\", line 184, in __init__\n",
      "    assert all(\n",
      "AssertionError: Observation spaces for all agents must be identical. Perhaps SuperSuit's pad_observations wrapper can help (useage: `supersuit.aec_wrappers.pad_observations(env)`\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::PPO.__init__()\u001b[39m (pid=25428, ip=127.0.0.1, repr=PPO)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 850, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 902, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 803, in ray._raylet.execute_task.function_executor\n",
      "  File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 674, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py\", line 445, in __init__\n",
      "    super().__init__(\n",
      "  File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\tune\\trainable\\trainable.py\", line 169, in __init__\n",
      "    self.setup(copy.deepcopy(self.config))\n",
      "  File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 466, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py\", line 571, in setup\n",
      "    self.workers = WorkerSet(\n",
      "  File \"D:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\", line 192, in __init__\n",
      "    raise e.args[0].args[2]\n",
      "AssertionError: Observation spaces for all agents must be identical. Perhaps SuperSuit's pad_observations wrapper can help (useage: `supersuit.aec_wrappers.pad_observations(env)`\n",
      "\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [PPO_pistonball_v6_c9148_00000])",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 29\u001b[0m\n\u001b[0;32m      5\u001b[0m ModelCatalog\u001b[38;5;241m.\u001b[39mregister_custom_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCNNModelV2\u001b[39m\u001b[38;5;124m\"\u001b[39m, CNNModelV2)\n\u001b[0;32m      7\u001b[0m config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m      8\u001b[0m     PPOConfig()\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;241m.\u001b[39mrollouts(num_rollout_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, rollout_fragment_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;241m.\u001b[39mresources(num_gpus\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRLLIB_NUM_GPUS\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[0;32m     27\u001b[0m )\n\u001b[1;32m---> 29\u001b[0m \u001b[43mtune\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPPO\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPPO\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtimesteps_total\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5000000\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m~/ray_results/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43menv_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Study_Documents\\thesis\\env\\lib\\site-packages\\ray\\tune\\tune.py:792\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, chdir_to_trial_dir, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, reuse_actors, raise_on_failed_trial, callbacks, max_concurrent_trials, trial_executor, _experiment_checkpoint_dir, _remote, _remote_string_queue)\u001b[0m\n\u001b[0;32m    790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m incomplete_trials:\n\u001b[0;32m    791\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raise_on_failed_trial \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m experiment_interrupted_event\u001b[38;5;241m.\u001b[39mis_set():\n\u001b[1;32m--> 792\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TuneError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrials did not complete\u001b[39m\u001b[38;5;124m\"\u001b[39m, incomplete_trials)\n\u001b[0;32m    793\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    794\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrials did not complete: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, incomplete_trials)\n",
      "\u001b[1;31mTuneError\u001b[0m: ('Trials did not complete', [PPO_pistonball_v6_c9148_00000])"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env_name = \"pistonball_v6\"\n",
    "\n",
    "    register_env(env_name, lambda config: ParallelPettingZooEnv(env_creator(config)))\n",
    "    ModelCatalog.register_custom_model(\"CNNModelV2\", CNNModelV2)\n",
    "\n",
    "    config = (\n",
    "        PPOConfig()\n",
    "        .rollouts(num_rollout_workers=4, rollout_fragment_length='auto')\n",
    "        .training(\n",
    "            train_batch_size=512,\n",
    "            lr=2e-5,\n",
    "            gamma=0.99,\n",
    "            lambda_=0.9,\n",
    "            use_gae=True,\n",
    "            clip_param=0.4,\n",
    "            grad_clip=None,\n",
    "            entropy_coeff=0.1,\n",
    "            vf_loss_coeff=0.25,\n",
    "            sgd_minibatch_size=64,\n",
    "            num_sgd_iter=10,\n",
    "        )\n",
    "        .environment(env=env_name, clip_actions=True)\n",
    "        .debugging(log_level=\"ERROR\")\n",
    "        .framework(framework=\"torch\")\n",
    "        .resources(num_gpus=int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")))\n",
    "    )\n",
    "\n",
    "    tune.run(\n",
    "        \"PPO\",\n",
    "        name=\"PPO\",\n",
    "        stop={\"timesteps_total\": 5000000},\n",
    "        checkpoint_freq=10,\n",
    "        local_dir=\"~/ray_results/\" + env_name,\n",
    "        config=config.to_dict(),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84133e9-b43c-46fa-a5b8-ae51e1053ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
