{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haven't read it yet but might be useful: https://arxiv.org/pdf/1904.06979.pdf\n",
    "\n",
    "Using Welch's t-test for now\n",
    "TODO: confirm if it's right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_rewards = np.asarray([-0.77648341, -0.77648341,  0.        , -0.84545385, -0.60473464,\n",
    "       -0.60473464,  0.        , -0.62770281, -0.83607652, -0.83607652,\n",
    "        0.        , -0.81167945, -0.50230277, -0.50230277,  0.        ,\n",
    "       -0.50768057, -0.37690899, -0.37690899,  0.        , -0.37139904,\n",
    "       -0.76003052, -0.76003052,  0.        , -0.71406728, -0.63670099,\n",
    "       -0.63670099,  0.        , -0.62312957, -0.58986421, -0.58986421,\n",
    "        0.        , -0.65778797, -0.15371678, -0.15371678,  0.        ,\n",
    "       -0.17047249, -0.74463809, -0.74463809,  0.        , -0.72543085,\n",
    "       -0.32895744, -0.32895744,  0.        , -0.36301456, -0.78510779,\n",
    "       -0.78510779,  0.        , -0.82559714])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = int(len(episode_rewards)/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1 = episode_rewards.reshape((eps,4)).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample2 = episode_rewards.reshape((eps,4)).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis\n",
    "# Null Hypothesis is that reward from a model trained on one strategy is performs the same on the other strategy that it was not trained on\n",
    "# H0: μ1 = μ2\n",
    "# Alternate Hypothesis is that reward is the same, i.e. strategy didn't have any effect on training\n",
    "# Ha: μ2 ≠ μ1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, p = scipy.stats.ttest_ind(\n",
    "    sample1,\n",
    "    sample2, \n",
    "    equal_var=False # Not sure\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s,p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "strats = [\"evasive\", \"hiding\", \"shifty\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_dict = {s: {s: None for s in strats} for s in strats}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mepe_vs_mepe': None,\n",
       " 'mepe_vs_meph': None,\n",
       " 'mepe_vs_meps': None,\n",
       " 'meph_vs_mepe': None,\n",
       " 'meph_vs_meph': None,\n",
       " 'meph_vs_meps': None,\n",
       " 'meps_vs_mepe': None,\n",
       " 'meps_vs_meph': None,\n",
       " 'meps_vs_meps': None,\n",
       " 'mhpe_vs_mhpe': None,\n",
       " 'mhpe_vs_mhph': None,\n",
       " 'mhpe_vs_mhps': None,\n",
       " 'mhph_vs_mhpe': None,\n",
       " 'mhph_vs_mhph': None,\n",
       " 'mhph_vs_mhps': None,\n",
       " 'mhps_vs_mhpe': None,\n",
       " 'mhps_vs_mhph': None,\n",
       " 'mhps_vs_mhps': None,\n",
       " 'mspe_vs_mspe': None,\n",
       " 'mspe_vs_msph': None,\n",
       " 'mspe_vs_msps': None,\n",
       " 'msph_vs_mspe': None,\n",
       " 'msph_vs_msph': None,\n",
       " 'msph_vs_msps': None,\n",
       " 'msps_vs_mspe': None,\n",
       " 'msps_vs_msph': None,\n",
       " 'msps_vs_msps': None}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = {}\n",
    "for model_strat in strats:\n",
    "    for s1_player_strat in strats:\n",
    "            for s2_player_strat in strats:\n",
    "                sample1 = reward_dict[model_strat][s1_player_strat]\n",
    "                sample2 = reward_dict[model_strat][s2_player_strat]\n",
    "                stats.update({f\"m{model_strat[0]}p{s1_player_strat[0]}_vs_m{model_strat[0]}p{s2_player_strat[0]}\": None})\n",
    "stats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
