{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Load the \"autoreload\" extension so that code can change\n",
    "%load_ext autoreload\n",
    "\n",
    "# OPTIONAL: always reload modules so that as you change code in src, it gets loaded\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
    "from rl.policy import EpsGreedyQPolicy, LinearAnnealedPolicy, BoltzmannQPolicy, MaxBoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.agents import DQNAgent\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Convolution2D, Dense, Flatten\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_creator(render_mode=\"rgb_array\", cycles=200):\n",
    "    from src.world import world_utils\n",
    "    env = world_utils.env(render_mode=render_mode, max_cycles=cycles)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(env):\n",
    "    height, width, channels = env.observation_space.shape\n",
    "    actions = env.action_space.n\n",
    "\n",
    "    print(height, width, channels)\n",
    "    print(actions)\n",
    "    print(env.unwrapped.get_action_meanings())\n",
    "\n",
    "    # 34,812,326 parameters\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, (8, 8), strides=(4, 4), activation='relu', input_shape=(FRAMES, height, width, channels)))\n",
    "    model.add(Convolution2D(64, (4, 4), strides=(2, 2), activation='relu'))\n",
    "    model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(actions, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    if POLICY == \"boltzman\":\n",
    "        policy = BoltzmannQPolicy()\n",
    "    elif POLICY == \"max\":\n",
    "        policy = MaxBoltzmannQPolicy()\n",
    "    elif POLICY == \"annealed\":\n",
    "        policy = LinearAnnealedPolicy(EpsGreedyQPolicy(\n",
    "        ), attr='eps', value_max=1., value_min=.1, value_test=.2, nb_steps=NB_STEPS)\n",
    "    elif POLICY == \"greedy\":\n",
    "        policy = EpsGreedyQPolicy()\n",
    "    else:\n",
    "        raise ValueError\n",
    "    memory = SequentialMemory(limit=FRAME_LIMIT, window_length=FRAMES)\n",
    "    agent = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                     enable_dueling_network=True, dueling_type='avg',\n",
    "                     nb_actions=actions, nb_steps_warmup=NB_STEPS_WARMUP\n",
    "                     )\n",
    "    agent.compile(Adam(lr=LEARNING_RATE), metrics=['mae'])\n",
    "    return agent\n",
    "\n",
    "def load_agent(env, filename):\n",
    "    model = build_model(env)\n",
    "    # model.summary()\n",
    "    actions = env.action_space.n\n",
    "    agent = build_agent(model, actions)\n",
    "    agent.load_weights(filename)\n",
    "    return agent\n",
    "\n",
    "def train_agent(env, vis=False, agent=None):\n",
    "    model = build_model(env)\n",
    "    # model.summary()\n",
    "    actions = env.action_space.n\n",
    "    if agent is None:\n",
    "        agent = build_agent(model, actions)\n",
    "        cont_prefix = \"\"\n",
    "    else:\n",
    "        cont_prefix = CONT_PREFIX\n",
    "\n",
    "    weights_filename = 'agents/dqn_' + cont_prefix + MODEL_TYPE+ '_' + POLICY + '_weights.h5f'\n",
    "    checkpoint_weights_filename = 'agents/dqn_'+ cont_prefix + \\\n",
    "        MODEL_TYPE + '_' + POLICY + '_weights_{step}.h5f'\n",
    "    log_filename = f'agents/dqn_{cont_prefix}{MODEL_TYPE}_{POLICY}_log.json'\n",
    "\n",
    "    callbacks = [ModelIntervalCheckpoint(\n",
    "        checkpoint_weights_filename, interval=5000)]\n",
    "    callbacks += [FileLogger(log_filename, interval=100)]\n",
    "\n",
    "    agent.fit(env, callbacks=callbacks, nb_steps=NB_STEPS,\n",
    "              visualize=vis, verbose=1, log_interval=1000)\n",
    "    agent.save_weights(weights_filename, overwrite=True)\n",
    "    return agent\n",
    "\n",
    "def continue_train_agent(env, filename, vis=False):\n",
    "    agent = load_agent(env,filename)\n",
    "    agent = train_agent(env, vis=vis, agent=agent)\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(env, agent, eps=20, vis=False):\n",
    "    scores = agent.test(env, nb_episodes=eps, visualize=vis)\n",
    "    print(np.mean(scores.history['episode_reward']))\n",
    "    return scores\n",
    "\n",
    "def record_video(eps=50):\n",
    "    rank_df = pd.read_csv(\"ranking_path.csv\")\n",
    "    recorded_df = pd.read_csv(\"recorded.csv\")\n",
    "    already = list(recorded_df['path'])\n",
    "\n",
    "    add_to_file = True\n",
    "\n",
    "    file = None\n",
    "\n",
    "    for item in rank_df['path']:\n",
    "        env = gym.make(ENV_NAME)\n",
    "        if item in already:\n",
    "            continue\n",
    "        try:\n",
    "            agent = load_agent(env, item)\n",
    "            scores = evaluate_agent(env, agent, eps=eps, vis=True)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Stopped\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            add_to_file = False\n",
    "\n",
    "        print(\"File: \",item)\n",
    "        path = []\n",
    "        done = []\n",
    "        path.append(item)\n",
    "        done.append(True)\n",
    "        new_recorded_df = pd.DataFrame({'path': path, 'done': done})\n",
    "        if add_to_file:\n",
    "            new_recorded_df.to_csv(\"recorded.csv\", index=False, mode='a', header=False)\n",
    "        env.close()\n",
    "        break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_visual_tests(env, eps=10):\n",
    "    rank_df = pd.read_csv(\"ranking_path.csv\")\n",
    "    note_df = pd.read_csv(\"notes.csv\")\n",
    "    already = list(note_df['path'])\n",
    "\n",
    "    path = []\n",
    "    note = []\n",
    "\n",
    "    for item in rank_df['path']:\n",
    "        if item in already:\n",
    "            continue\n",
    "        print(\"File: \",item)\n",
    "        try:\n",
    "            agent = load_agent(env, item)\n",
    "            scores = evaluate_agent(env, agent, eps=eps, vis=True)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Stopped\")\n",
    "            path.append(item)\n",
    "            note.append(input(\"Enter notes: \"))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            new_note_df = pd.DataFrame({'path': path, 'note': note})\n",
    "            new_note_df.to_csv(\"notes.csv\", index=False, mode='a', header=False)\n",
    "            return\n",
    "\n",
    "        if input('Do You Want To Continue? ') != 'y':\n",
    "            break\n",
    "\n",
    "    new_note_df = pd.DataFrame({'path': path, 'note': note})\n",
    "    new_note_df.to_csv(\"notes.csv\", index=False, mode='a', header=False)\n",
    "\n",
    "def run_tests(env, eps=20, vis=False):\n",
    "    models = [x[:-6] for x in glob.glob(\"./agents/*.h5f.index\")]\n",
    "\n",
    "    for filename in models:\n",
    "        print(filename)\n",
    "        agent = load_agent(env, filename)\n",
    "        scores = evaluate_agent(env, agent, eps=eps, vis=vis)\n",
    "        df = pd.DataFrame(scores.history)\n",
    "        df[\"model\"] = filename\n",
    "        if exists(\"results.csv\"):\n",
    "            df.to_csv(\"results.csv\", index=False, mode='a', header=False)\n",
    "        else:\n",
    "            df.to_csv(\"results.csv\", index=False)\n",
    "\n",
    "def run_tests_new(env, eps=30, vis=False):\n",
    "    #models = [x[:-6] for x in glob.glob('./agents/final/*.h5f.index')]\n",
    "    #names = [x[15:-4] for x in models]\n",
    "\n",
    "    already = []\n",
    "\n",
    "    if exists(\"results.csv\"):\n",
    "        results = pd.read_csv(\"results.csv\")\n",
    "        already =  results['model'].unique()\n",
    "\n",
    "    # models = [x[:-6] for x in glob.glob('./agents/Done/100k_greedy/*.h5f.index')][::-1]\n",
    "    # names = [x[x.rindex('/',)+1:-4] for x in models]\n",
    "\n",
    "    models = [x[:-6] for x in glob.glob('./agents/Done/**/*.h5f.index', recursive=True)]\n",
    "    names = [x[x.index('Done')+5:-4] for x in models]\n",
    "\n",
    "    for i in range(len(models)):\n",
    "        if names[i] in already:\n",
    "            continue\n",
    "        agent = load_agent(env, models[i])\n",
    "        print(names[i])\n",
    "        scores = evaluate_agent(env, agent, eps=eps, vis=vis)\n",
    "        df = pd.DataFrame(scores.history)\n",
    "        df[\"model\"] = names[i]\n",
    "        if exists(\"results.csv\"):\n",
    "            df.to_csv(\"results.csv\", index=False, mode='a', header=False)\n",
    "        else:\n",
    "            df.to_csv(\"results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_logs():\n",
    "    if not exists(\"training.csv\"):\n",
    "        jsons = glob.glob(\"./agents/final/*.json\")\n",
    "\n",
    "        results = []\n",
    "        for filename in jsons:\n",
    "            with open(filename, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                df = pd.DataFrame(data)\n",
    "                df[\"model\"] = filename\n",
    "                results.append(df)\n",
    "        results_df = pd.concat(results, ignore_index=True)\n",
    "        results_df.to_csv(\"training.csv\")\n",
    "    else:\n",
    "        raise FileExistsError\n",
    "\n",
    "\n",
    "def visualize_log(filename, figsize=None, output=None):\n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    if 'episode' not in data:\n",
    "        raise ValueError(\n",
    "            f'Log file \"{filename}\" does not contain the \"episode\" key.')\n",
    "    episodes = data['episode']\n",
    "\n",
    "    # Get value keys. The x axis is shared and is the number of episodes.\n",
    "    keys = sorted(list(set(data.keys()).difference({'episode'})))\n",
    "\n",
    "    if figsize is None:\n",
    "        figsize = (15., 5. * len(keys))\n",
    "    f, axarr = plt.subplots(len(keys), sharex=True, figsize=figsize)\n",
    "    for idx, key in enumerate(keys):\n",
    "        axarr[idx].plot(episodes, data[key])\n",
    "        axarr[idx].set_ylabel(key)\n",
    "    plt.xlabel('episodes')\n",
    "    plt.tight_layout()\n",
    "    if output is None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_main = \"\"\n",
    "env = env_creator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adversary_0': Box(-inf, inf, (34,), float32),\n",
       " 'adversary_1': Box(-inf, inf, (34,), float32),\n",
       " 'adversary_2': Box(-inf, inf, (34,), float32),\n",
       " 'agent_0': Box(-inf, inf, (34,), float32)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gymnasium.spaces.discrete.Discrete"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(env.action_spaces['agent_0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#height, width, channels = env.observation_space.shape\n",
    "#actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_main == \"train\":\n",
    "        agent = train_agent(env, vis=args.vis)\n",
    "    elif run_main == \"continue\":\n",
    "        agent= continue_train_agent(env, args.file, vis=args.vis)\n",
    "    elif run_main == \"eval\":\n",
    "        agent = load_agent(env, args.file)\n",
    "        # agent = load_agent(env,'./agents/dqn_Qbert-v0_weights_1000.h5f')\n",
    "        evaluate_agent(env, agent, vis=args.vis)\n",
    "    elif run_main == \"plot\":\n",
    "        visualize_log(args.file)\n",
    "    elif run_main == \"test\":\n",
    "        run_tests(env, eps=1, vis=args.vis)\n",
    "    elif run_main == \"testnew\":\n",
    "        run_tests_new(env, eps=30, vis=args.vis)\n",
    "    elif run_main == \"vtest\":\n",
    "        run_visual_tests(env)\n",
    "    elif run_main == \"record\":\n",
    "        record_video()\n",
    "    elif run_main == \"combine\":\n",
    "        combine_logs()\n",
    "    else:\n",
    "        print(\"Incorrect args\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "07a8de9a3e4fc31eb906ba57dde9d36a5d68d35f33988173204ce5b09d40b02e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
